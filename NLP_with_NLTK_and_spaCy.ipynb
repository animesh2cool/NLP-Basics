{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlYsdN3Xji+oobfCmHi7kK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/animesh2cool/NLP-Basics/blob/main/NLP_with_NLTK_and_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Steps :\n",
        "\n",
        "1. Segmentation\n",
        "2. Tokenizing\n",
        "3. Removing Stop Words\n",
        "4. Stemming\n",
        "5. Lemmatization\n",
        "6. Part of Speech Tagging\n",
        "7. Named Entity Tagging"
      ],
      "metadata": {
        "id": "5XYAhfw0mdvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Segmentation**"
      ],
      "metadata": {
        "id": "LohkkA3IofuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "\n",
        "# import sentence tokenizer lib\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIM0qBavofGP",
        "outputId": "8cb06579-0dd8-425b-d96a-cf6855d69acc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create sentence Tokenizer function\n",
        "\n",
        "def sentence_tokenizer(text):\n",
        "  return sent_tokenize(text)"
      ],
      "metadata": {
        "id": "XQ-clIxWoykf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading text\n",
        "text=\"The league stage will be played across 12 venues in India, while the playoffs phase of the schedule is yet to be announced. The Assam Cricket Association Stadium, Guwahati will be making its IPL debut and will host the first two Rajasthan Royals home games before they play their remaining five home games at Sawai Mansingh Stadium in Jaipur,[21] while Dharamsala will be making its return to IPL after ten years and will host the final two Punjab Kings home games after they play their first five home games at Mohali. The other eight teams will be playing all their home games at their traditional home grounds. The home stadiums of all IPL teams are Ahmedabad, Mohali, Lucknow, Bangalore, Chennai, Delhi, Kolkata, Jaipur, and Mumbai. Guwahati and Dharamshala are the 2 new venues added in IPL 2023.\""
      ],
      "metadata": {
        "id": "chv9WkMCo30H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizing sentence and printing each sentece\n",
        "sentence=sentence_tokenizer(text)\n",
        "for i in sentence:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQhqVT2Lo8k1",
        "outputId": "bd7bb19a-f027-42e6-f78a-7acb3efce745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The league stage will be played across 12 venues in India, while the playoffs phase of the schedule is yet to be announced.\n",
            "The Assam Cricket Association Stadium, Guwahati will be making its IPL debut and will host the first two Rajasthan Royals home games before they play their remaining five home games at Sawai Mansingh Stadium in Jaipur,[21] while Dharamsala will be making its return to IPL after ten years and will host the final two Punjab Kings home games after they play their first five home games at Mohali.\n",
            "The other eight teams will be playing all their home games at their traditional home grounds.\n",
            "The home stadiums of all IPL teams are Ahmedabad, Mohali, Lucknow, Bangalore, Chennai, Delhi, Kolkata, Jaipur, and Mumbai.\n",
            "Guwahati and Dharamshala are the 2 new venues added in IPL 2023.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "XS0q2vhNASQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnnizlFgwSyr"
      },
      "outputs": [],
      "source": [
        "s1=\"Tokenization is essential in NLP.\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "8W_vNOPdy6Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "n82mOsq4zA8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(s1)"
      ],
      "metadata": {
        "id": "J1clzh8ozE0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP steps in Spacy including spacy inbuild functions\n",
        "\n",
        "nlp.pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9SWDe286Gqv",
        "outputId": "37ad92a2-53d1-4685-ce2a-62c02b0b5498"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x79efb70c1cc0>),\n",
              " ('tagger', <spacy.pipeline.tagger.Tagger at 0x79efb70c2680>),\n",
              " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x79f0693c74c0>),\n",
              " ('attribute_ruler',\n",
              "  <spacy.pipeline.attributeruler.AttributeRuler at 0x79efb7b5cc40>),\n",
              " ('lemmatizer',\n",
              "  <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x79efb6e78c40>),\n",
              " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x79efb70ab6f0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP steps in Spacy\n",
        "\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZunFUci86UAw",
        "outputId": "1c82d9ad-9850-476f-da15-a19f6b7e477f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XkhwVhD6gaX",
        "outputId": "65d17691-cca2-485b-88b9-608fa3e0f0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenization\n",
            "is\n",
            "essential\n",
            "in\n",
            "NLP\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing complex words from the text\n",
        "\n",
        "s2 = \"I found a great resourse at https://example.com for learning NLP or email me at example@gmail.com\""
      ],
      "metadata": {
        "id": "_MI1bJkG6unC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(s2)"
      ],
      "metadata": {
        "id": "1G6JHySZ7JT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc2:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_wH6uC87MVZ",
        "outputId": "287de8a0-62de-4599-dc87-bc7e00453d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "found\n",
            "a\n",
            "great\n",
            "resourse\n",
            "at\n",
            "https://example.com\n",
            "for\n",
            "learning\n",
            "NLP\n",
            "or\n",
            "email\n",
            "me\n",
            "at\n",
            "example@gmail.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9OHUHgv7QWX",
        "outputId": "b89f5ea6-76af-4ad5-f0b4-0fa4368a7013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove stop wodrs"
      ],
      "metadata": {
        "id": "-3yM_UpO_zMI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl_3c7ax8dd0",
        "outputId": "b290fe93-258f-4c8c-b2ed-99af63222770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'every', 'still', 'noone', 'whereby', 'itself', 'eight', 'their', 'really', 'hers', 'those', 'while', '’ll', 'whereupon', '‘d', 'amount', '‘ll', 'except', 'was', 'say', 'whether', 'latterly', 'on', 'between', 'whose', 'did', 'become', 'thereby', 'afterwards', 'himself', 'well', 'with', 'sixty', 'until', 'been', 'using', 'four', 'six', 'would', 'yourself', 'mostly', 'besides', 'all', 'made', 'seems', 'nine', 'me', 'how', 'move', 'none', '’re', 'former', 'no', 'rather', 'regarding', 'most', 'otherwise', 'ever', 'ourselves', 'whence', 'an', 'any', 'can', 'had', 'give', 'anyhow', 'namely', 'twelve', 'became', 'few', 'from', 'hundred', 'less', '‘ve', 'herself', 'after', 'sometime', 'should', 'beyond', 'also', 'along', 'already', 'both', 'another', 'over', 'becoming', 'top', 'my', 'down', 'during', 'else', 'formerly', 'am', 'forty', 'thus', 'i', 'per', 'what', 'hence', 'off', 'go', 'each', 'however', 'cannot', 'yourselves', 'yours', 'three', 'call', 'which', \"'d\", 'ten', 'of', 'nor', 'whither', 'nowhere', 'wherein', 'third', 'yet', 'the', 'although', 'toward', 'same', 'these', 'who', 'even', 'could', 'before', 'are', 'more', '‘m', '’s', 'fifteen', 'your', 'whereas', 'through', 'thru', 'there', 'put', 'as', 'around', 'everything', 'least', 'whenever', 'above', 'onto', 'into', 'will', 'why', 'being', 'almost', 'is', 'eleven', 'themselves', 'some', 'quite', 'wherever', 'herein', 'never', 'throughout', 'two', 'front', 'via', 'nobody', 'someone', '‘s', 'elsewhere', 'does', 'first', 'because', 'just', 'show', 'due', 'indeed', 'once', 'side', 'take', 'towards', 'if', 'further', 'they', 'therefore', 'has', 'hereafter', 'you', '’ve', 'see', \"'re\", 'but', 'somewhere', 'either', \"'m\", 'anyone', 'against', 'here', 'its', 'please', 'across', 'back', 'our', 'anything', 'full', 'not', 'myself', 'about', '‘re', 'whereafter', 'whom', 'enough', 'many', 'get', 'have', 'be', 'whole', 'latter', 'serious', 'done', 'everyone', 'at', 'alone', 're', 'that', 'keep', 'than', 'he', 'hereby', 'something', 'his', 'nevertheless', 'whatever', 'hereupon', 'sometimes', 'though', 'twenty', 'becomes', 'part', 'us', 'n‘t', 'ours', 'make', 'might', 'thereafter', 'beside', 'therein', 'again', 'amongst', 'whoever', 'together', 'such', 'somehow', 'very', 'and', 'to', 'mine', 'since', 'up', 'moreover', 'next', 'seeming', 'must', 'this', 'perhaps', 'neither', 'by', 'in', 'anywhere', 'them', \"'ll\", 'her', 'do', 'five', 'often', 'a', 'seem', 'nothing', 'without', 'out', 'for', 'everywhere', 'him', 'unless', 'or', 'may', 'thereupon', 'n’t', 'empty', 'only', 'own', 'doing', 'last', 'under', 'anyway', '’m', 'it', 'she', 'always', 'within', 'seemed', \"n't\", 'several', 'were', \"'ve\", 'others', '’d', 'below', 'various', \"'s\", 'other', 'when', 'meanwhile', 'we', 'beforehand', 'among', 'then', 'much', 'name', 'now', 'thence', 'too', 'upon', 'one', 'used', 'ca', 'so', 'bottom', 'behind', 'fifty', 'where'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl7fa64hAEyj",
        "outputId": "93acd4c8-c414-4e94-a424-99731453aa5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab['the'].is_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X717PM3aAOA4",
        "outputId": "4bf217c1-70d0-46a5-8a71-891fb8efe659"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab['good'].is_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUagQ5PxAiBI",
        "outputId": "0ffd85bf-8465-40df-f8ea-18e43148da57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s5=\"This is an example of stop Word Removal with some Words that we want to remove\""
      ],
      "metadata": {
        "id": "Jhqd094zAlSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc5 = nlp(s5)"
      ],
      "metadata": {
        "id": "ctzkY0GwAzxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_without_stopwords = [token.text for token in doc5 if token.text.lower() not in nlp.Defaults.stop_words]"
      ],
      "metadata": {
        "id": "Kl-eQxVWA8Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_without_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj6odkXsBeix",
        "outputId": "2dc27884-ce0e-4682-b969-7ef42d182a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['example', 'stop', 'Word', 'Removal', 'Words', 'want', 'remove']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#injecting external stopword into default stopwords\n",
        "nlp.Defaults.stop_words.add('example')"
      ],
      "metadata": {
        "id": "sz5brMGEBor3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_without_stopwords = [token.text for token in doc5 if token.text.lower() not in nlp.Defaults.stop_words]"
      ],
      "metadata": {
        "id": "nh1srp3UCWzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_without_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HHYURsaCi-F",
        "outputId": "7457b491-8b31-4215-cbba-ad246befdccf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stop', 'Word', 'Removal', 'Words', 'want', 'remove']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords from default stopword\n",
        "\n",
        "nlp.Defaults.stop_words.remove('example')"
      ],
      "metadata": {
        "id": "7ayGq3AMClHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_without_stopwords = [token.text for token in doc5 if token.text.lower() not in nlp.Defaults.stop_words]"
      ],
      "metadata": {
        "id": "hv4u6YqMCziE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_without_stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlyDaCOPC1g5",
        "outputId": "a7ae8e47-08c2-4c65-eaca-f7802b131fee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['example', 'stop', 'Word', 'Removal', 'Words', 'want', 'remove']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming and Lemmatization"
      ],
      "metadata": {
        "id": "exUT16AsDrpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "H5db2RbXC2xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()"
      ],
      "metadata": {
        "id": "HJ1wJe4FDO5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"start\",'starts','started','starting']"
      ],
      "metadata": {
        "id": "KrZbKn7hDSI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(f\"{word}--->{porter.stem(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5I63Bi4Db_f",
        "outputId": "83a34860-82d9-4965-a499-a7ec22d892ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start--->start\n",
            "starts--->start\n",
            "started--->start\n",
            "starting--->start\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"run\",'running','runs','runner','starting','files','better','fairly','mouse','cats','happily']"
      ],
      "metadata": {
        "id": "7PQIBoJ7Dy0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(f\"{word}--->{porter.stem(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfhasbTOESxK",
        "outputId": "e31b7c39-3286-4e5f-99d6-affed005cdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run--->run\n",
            "running--->run\n",
            "runs--->run\n",
            "runner--->runner\n",
            "starting--->start\n",
            "files--->file\n",
            "better--->better\n",
            "fairly--->fairli\n",
            "mouse--->mous\n",
            "cats--->cat\n",
            "happily--->happili\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "6uOL9kyMEVT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(language='english')"
      ],
      "metadata": {
        "id": "GwYUQA5nFIGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(f\"{word}--->{snowball.stem(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVOpFWmMFJlC",
        "outputId": "ce1a28a7-c2bc-4bd9-e022-a9f2f508bcf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run--->run\n",
            "running--->run\n",
            "runs--->run\n",
            "runner--->runner\n",
            "starting--->start\n",
            "files--->file\n",
            "better--->better\n",
            "fairly--->fair\n",
            "mouse--->mous\n",
            "cats--->cat\n",
            "happily--->happili\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lematization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "VwGiLHjkFPVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDcTEn4KGJ2g",
        "outputId": "5af11408-22e7-4d34-bc6e-dc79775203d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordnet = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "dz41uhc3GL9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(f\"{word}--->{wordnet.lemmatize(word)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bLErt_fGcNO",
        "outputId": "d5f1a3f6-268e-4524-f79b-a5dfc4fec10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run--->run\n",
            "running--->running\n",
            "runs--->run\n",
            "runner--->runner\n",
            "starting--->starting\n",
            "files--->file\n",
            "better--->better\n",
            "fairly--->fairly\n",
            "mouse--->mouse\n",
            "cats--->cat\n",
            "happily--->happily\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "IEy4XtmYGinX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "m4zmCv1qG1L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\" \".join(words))"
      ],
      "metadata": {
        "id": "9LDv7LjCG3B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(f\"{token}--->{token.lemma_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b0xtv7rHFkw",
        "outputId": "9dd450c5-db25-4591-9e42-193c1f2f7425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run--->run\n",
            "running--->running\n",
            "runs--->run\n",
            "runner--->runner\n",
            "starting--->start\n",
            "files--->file\n",
            "better--->well\n",
            "fairly--->fairly\n",
            "mouse--->mouse\n",
            "cats--->cat\n",
            "happily--->happily\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pos tagging"
      ],
      "metadata": {
        "id": "G2rdeMJ7Hs0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"The Sun shines brigh, and sets in the West.\"\n"
      ],
      "metadata": {
        "id": "KGGrjQDSHK-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(sent)"
      ],
      "metadata": {
        "id": "BKXc4DKao2Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token\\t\\tPOS\\t\\tDetailed POS\\t\\tExplanation\")\n",
        "print(\"_______________________________________________________________________________\")\n",
        "for token in doc:\n",
        "  print(f\"{token.text}\\t\\t{token.pos_}\\t\\t{token.tag_}\\t\\t{spacy.explain(token.tag_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FKRk7WCo3nO",
        "outputId": "3b0a9c18-bfc2-4aab-db99-0a97b2ae7ed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token\t\tPOS\t\tDetailed POS\t\tExplanation\n",
            "_______________________________________________________________________________\n",
            "The\t\tDET\t\tDT\t\tdeterminer\n",
            "Sun\t\tPROPN\t\tNNP\t\tnoun, proper singular\n",
            "shines\t\tVERB\t\tVBZ\t\tverb, 3rd person singular present\n",
            "brigh\t\tNOUN\t\tNN\t\tnoun, singular or mass\n",
            ",\t\tPUNCT\t\t,\t\tpunctuation mark, comma\n",
            "and\t\tCCONJ\t\tCC\t\tconjunction, coordinating\n",
            "sets\t\tNOUN\t\tNNS\t\tnoun, plural\n",
            "in\t\tADP\t\tIN\t\tconjunction, subordinating or preposition\n",
            "the\t\tDET\t\tDT\t\tdeterminer\n",
            "West\t\tPROPN\t\tNNP\t\tnoun, proper singular\n",
            ".\t\tPUNCT\t\t.\t\tpunctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key,val in doc.count_by(spacy.attrs.POS).items():\n",
        "    print(f\"{key} {doc.vocab[key].text} {val}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwZha7oxpR4n",
        "outputId": "1a59e4f6-995e-493f-ca55-c64de73d875a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90 DET 2\n",
            "96 PROPN 2\n",
            "100 VERB 1\n",
            "92 NOUN 2\n",
            "97 PUNCT 2\n",
            "89 CCONJ 1\n",
            "85 ADP 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning determiner and punctuation\n",
        "\n",
        "pos_to_remove = ['DET','PUNCT']"
      ],
      "metadata": {
        "id": "W1ASepDAwL9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_doc = \" \".join(token.text for token in doc if token.pos_ not in pos_to_remove)"
      ],
      "metadata": {
        "id": "F4xcxyqpxeN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(clean_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjnvKHusxssm",
        "outputId": "8f8042c0-12c0-4db0-d639-73fd5c1b9e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun shines brigh and sets in West\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "nMr9v5J1xwe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(docs=doc,style='dep',options={'distance':100})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "tKt-Tvce0XSn",
        "outputId": "1616eb52-94ea-4484-e252-b52801b29361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"43dbb154a4224acda35afaeb2a497f35-0\" class=\"displacy\" width=\"950\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">Sun</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">shines</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">brigh,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">sets</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">West.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-0\" stroke-width=\"2px\" d=\"M70,152.0 C70,102.0 140.0,102.0 140.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,154.0 L62,142.0 78,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-1\" stroke-width=\"2px\" d=\"M170,152.0 C170,102.0 240.0,102.0 240.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M170,154.0 L162,142.0 178,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-2\" stroke-width=\"2px\" d=\"M270,152.0 C270,102.0 340.0,102.0 340.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M340.0,154.0 L348.0,142.0 332.0,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-3\" stroke-width=\"2px\" d=\"M270,152.0 C270,52.0 445.0,52.0 445.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M445.0,154.0 L453.0,142.0 437.0,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-4\" stroke-width=\"2px\" d=\"M270,152.0 C270,2.0 550.0,2.0 550.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M550.0,154.0 L558.0,142.0 542.0,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-5\" stroke-width=\"2px\" d=\"M570,152.0 C570,102.0 640.0,102.0 640.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M640.0,154.0 L648.0,142.0 632.0,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-6\" stroke-width=\"2px\" d=\"M770,152.0 C770,102.0 840.0,102.0 840.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M770,154.0 L762,142.0 778,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-43dbb154a4224acda35afaeb2a497f35-0-7\" stroke-width=\"2px\" d=\"M670,152.0 C670,52.0 845.0,52.0 845.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-43dbb154a4224acda35afaeb2a497f35-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M845.0,154.0 L853.0,142.0 837.0,142.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition**"
      ],
      "metadata": {
        "id": "EEK6Q9Ahtozb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Elon Musk is the CEO of Tesla. Born on June 28, 1971, in Pretoria, South Africa.\""
      ],
      "metadata": {
        "id": "3MWkWm0vu7YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "x7QAmhVWvYrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPbpPAe7v2V4",
        "outputId": "30071a65-e4bf-492c-9555-d62f131499da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Elon Musk, Tesla, June 28, 1971, Pretoria, South Africa)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text,\"-\",ent.label_,spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_cGj-sywqXE",
        "outputId": "c98af09f-74a8-4d1c-a229-6ab39552722a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk - PERSON People, including fictional\n",
            "Tesla - ORG Companies, agencies, institutions, etc.\n",
            "June 28, 1971 - DATE Absolute or relative dates or periods\n",
            "Pretoria - GPE Countries, cities, states\n",
            "South Africa - GPE Countries, cities, states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.get_pipe('ner').labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsGXy-yswwaB",
        "outputId": "fec27de9-ff27-4b97-d1c3-f01db8046fff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('CARDINAL',\n",
              " 'DATE',\n",
              " 'EVENT',\n",
              " 'FAC',\n",
              " 'GPE',\n",
              " 'LANGUAGE',\n",
              " 'LAW',\n",
              " 'LOC',\n",
              " 'MONEY',\n",
              " 'NORP',\n",
              " 'ORDINAL',\n",
              " 'ORG',\n",
              " 'PERCENT',\n",
              " 'PERSON',\n",
              " 'PRODUCT',\n",
              " 'QUANTITY',\n",
              " 'TIME',\n",
              " 'WORK_OF_ART')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name = None\n",
        "birth_date = None\n",
        "\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == 'PERSON':\n",
        "        name = ent.text\n",
        "    elif ent.label_ == 'DATE':\n",
        "        birth_date = ent.text\n",
        "\n",
        "print(f\"Name: {name}\")\n",
        "print(f\"Birth Date: {birth_date}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4enP6WXxPOU",
        "outputId": "b557f61a-d865-4afb-adba-3c5b90a3b0bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: Elon Musk\n",
            "Birth Date: June 28, 1971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = \"Facebook is a social media website\""
      ],
      "metadata": {
        "id": "RWVwYP5txv7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2 = nlp(text2)"
      ],
      "metadata": {
        "id": "p6OcbkbcyDE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2.ents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUHZwcRjyGdg",
        "outputId": "bade1fcc-df8f-4e3a-87b8-3eb5b7381444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "()"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.tokens import Span"
      ],
      "metadata": {
        "id": "-5KczuwWyOhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ORG = doc2.vocab.strings['ORG']"
      ],
      "metadata": {
        "id": "kgjGJKYXyXtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_ent = Span(doc2,0,1,label=ORG)"
      ],
      "metadata": {
        "id": "7ZjyRyksycc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc2.ents = list(doc2.ents) + [new_ent]"
      ],
      "metadata": {
        "id": "6CNr8ApQyswI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc2.ents:\n",
        "    print(ent.text,\"-\",ent.label_,\"-\",spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJR_xBoQy2gu",
        "outputId": "13ab31ae-01ec-4a77-ef28-6073533326b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Facebook - ORG - Companies, agencies, institutions, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "rmKHfpf_zt21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(docs=doc,style='ent',options={'distance':100},jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "Rnld-yt2z6qX",
        "outputId": "ff99e4d1-b1c5-4af5-ac62-f6b8788d6b0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Elon Musk\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is the CEO of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ". Born on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    June 28, 1971\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Pretoria\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    South Africa\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(docs=doc,style='ent',options={'ents':['PERSON','DATE']},jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Lg_X6mPTz_18",
        "outputId": "a7a86123-5aa2-485b-88d0-62bac79866be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Elon Musk\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is the CEO of Tesla. Born on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    June 28, 1971\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", in Pretoria, South Africa.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ao-o94Mg0m5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ZtY55AdbvTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNXMnmoRbvQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBcM-MJ7bvNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dB5Br5PCbvKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a0wzSVhxbu5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8g0psoObu2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8f9Lnhrbuzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yAMhcZeFbuuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA 1\n",
        "\n",
        "Lab Assignment:\n",
        "\n",
        "Build your own segmentation model for text to sentence and sentence to word.\n",
        "\n",
        "Develop a Python script using NLTK to parse a text file and identify parts of speech for each word.\n",
        "\n",
        "Build a Parts of speech tagger from look-up-table.\n",
        "\n",
        "Build a Parts of speech tagger from using N-gram model.\n",
        "\n",
        "Build a spell checker using edit distance algorithm for a limited vocabulary.\n",
        "\n",
        "Build a Parts of speech tagger using HMM model."
      ],
      "metadata": {
        "id": "8R2iekJNbdgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import a text paragraph\n",
        "\n",
        "text=\"Word embedding is a foundational technique in natural language processing (NLP) that transforms words into continuous vector representations in a fixed-dimensional space. Unlike traditional methods like one-hot encoding, which represent words as sparse vectors with single high-dimensional bits, word embeddings provide dense, low-dimensional vectors that capture semantic meanings and relationships. By mapping semantically similar words to nearby points in this vector space, embeddings enable more effective analysis of text data. Popular models for generating word embeddings include Word2Vec, GloVe (Global Vectors for Word Representation), and FastText, each with unique methods for learning these representations from large text corpora. Word embeddings facilitate a range of NLP applications such as text classification, sentiment analysis, and language translation by providing meaningful numerical representations of words that machine learning algorithms can easily process. This technique has significantly enhanced the ability to perform nuanced and sophisticated language understanding tasks.\""
      ],
      "metadata": {
        "id": "XwHBdBmqZv87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define function to segment text into sentence\n",
        "\n",
        "def text_to_sentences(text):\n",
        "    sentences = []\n",
        "    current_sentence = \"\"\n",
        "    for char in text:\n",
        "        if char in ['.', '!', '?',';']:\n",
        "            if current_sentence:\n",
        "                sentences.append(current_sentence.strip())\n",
        "                current_sentence = \"\"\n",
        "        else:\n",
        "            current_sentence += char\n",
        "    if current_sentence:\n",
        "        sentences.append(current_sentence.strip())\n",
        "    return sentences\n",
        "\n",
        "#pass text through text_to_sentences function and print each sentences\n",
        "\n",
        "sentences = text_to_sentences(text)\n",
        "print(\"Text to Sentences:\")\n",
        "for i in sentences:\n",
        "    print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNCTOfb3YB3z",
        "outputId": "6a7f592b-718d-4936-bd83-8f9c70d95880"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text to Sentences:\n",
            "Word embedding is a foundational technique in natural language processing (NLP) that transforms words into continuous vector representations in a fixed-dimensional space\n",
            "Unlike traditional methods like one-hot encoding, which represent words as sparse vectors with single high-dimensional bits, word embeddings provide dense, low-dimensional vectors that capture semantic meanings and relationships\n",
            "By mapping semantically similar words to nearby points in this vector space, embeddings enable more effective analysis of text data\n",
            "Popular models for generating word embeddings include Word2Vec, GloVe (Global Vectors for Word Representation), and FastText, each with unique methods for learning these representations from large text corpora\n",
            "Word embeddings facilitate a range of NLP applications such as text classification, sentiment analysis, and language translation by providing meaningful numerical representations of words that machine learning algorithms can easily process\n",
            "This technique has significantly enhanced the ability to perform nuanced and sophisticated language understanding tasks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NLTK library and download necessary NLTK data\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj6DTNWyYCAq",
        "outputId": "780d3cab-820c-4384-d501-956575dd1f21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to open text file, tokenize and pos tag\n",
        "def pos_taging_text_file(file_path):\n",
        "    # Read the text file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Perform POS tagging on words\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    return pos_tags"
      ],
      "metadata": {
        "id": "SzfuN0nRYBxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the text file from the local\n",
        "file_path = '/content/nlptest.txt'\n",
        "\n",
        "#pass text file through pos_taging_text_file function\n",
        "pos_tags = pos_taging_text_file(file_path)\n",
        "\n",
        "# Print each word with its POS tag\n",
        "for word, pos_tag in pos_tags:\n",
        "  print(f\"{word}: {pos_tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "-J8pEY5iYBuq",
        "outputId": "e8e40662-e96d-4992-cec6-12925defa039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/nlptest.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-313a59c079e9>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#pass text file through pos_taging_text_file function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpos_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_taging_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print each word with its POS tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-e368cabaa9be>\u001b[0m in \u001b[0;36mpos_taging_text_file\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpos_taging_text_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Read the text file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/nlptest.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define Parts of speech tagger funtion from look-up-table.\n",
        "\n",
        "# Define a simple look-up table (dictionary)\n",
        "pos_lookup = {\n",
        "    'I': 'PRP',\n",
        "    'am': 'VBP',\n",
        "    'going': 'VBG',\n",
        "    'to': 'TO',\n",
        "    'the': 'DT',\n",
        "    'park': 'NN',\n",
        "    'with': 'IN',\n",
        "    'my': 'PRP$',\n",
        "    'dog': 'NN'\n",
        "}\n",
        "\n",
        "def pos_tag_lookup(text1):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text1)\n",
        "\n",
        "    # Initialize an empty list to store (word, pos_tag) tuples\n",
        "    tagged_words = []\n",
        "\n",
        "    # Tag each word using the look-up table\n",
        "    for word in words:\n",
        "        # Use get() method to handle words not in the dictionary (defaulting to 'NN')\n",
        "        pos_tag = pos_lookup.get(word.lower(), 'NN')  # Convert to lowercase for case insensitivity\n",
        "        tagged_words.append((word, pos_tag))\n",
        "\n",
        "    return tagged_words\n",
        "\n",
        "#pass text paragraph through tagged words funtion\n",
        "tagged_words = pos_tag_lookup(text)\n",
        "\n",
        "# Print each word with its POS tag\n",
        "for word, pos_tag in tagged_words:\n",
        "  print(f\"{word}: {pos_tag}\")"
      ],
      "metadata": {
        "id": "Y_ENPQOzYBoV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}